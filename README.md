# servemd

**Serve docs to humans and AI.**

Beautiful markdown documentation with native llms.txt support. Zero configuration, production-ready.

[![PyPI](https://img.shields.io/pypi/v/servemd.svg)](https://pypi.org/project/servemd/)
[![Docker Hub](https://img.shields.io/docker/v/jberends/servemd?label=docker)](https://hub.docker.com/r/jberends/servemd)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.128+-green.svg)](https://fastapi.tiangolo.com/)
[![MCP](https://img.shields.io/badge/MCP-enabled-purple.svg)](docs/features/mcp.md)
[![Tests](https://img.shields.io/badge/tests-208%20passing-brightgreen.svg)](tests/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](LICENSE)

---

## Why servemd?

Unlike basic markdown servers, **servemd** is built for the AI era:

```
Markdown â†’ Beautiful HTML    â†’ Humans
         â†’ llms.txt          â†’ AI/LLMs
         â†’ llms-full.txt     â†’ Complete AI context
         â†’ /mcp endpoint     â†’ AI assistants (250x less context)
```

**For humans:** Nuxt UI-inspired design, three-column layout, zero configuration.
**For AI:** Native llms.txt support, structured context, ready for the Model Context Protocol era.

---

## âœ¨ Features

- ðŸŽ¨ **Beautiful Design** â€” Nuxt UI-inspired three-column layout (sidebar, content, TOC)
- ðŸ¤– **AI-Native** â€” Built-in llms.txt and llms-full.txt for Claude, ChatGPT, Cursor, etc.
- ðŸ”Œ **MCP Support** â€” Model Context Protocol endpoint for interactive AI queries (250x less context)
- âœ¨ **Zero Configuration** â€” Drop `.md` files and go
- âš¡ **Fast** â€” Smart disk caching, <5ms cached responses
- ðŸ³ **Docker Ready** â€” Production-optimized container
- ðŸ§ª **Well Tested** â€” 208 tests, 100% passing
- ðŸ“± **Responsive** â€” Mobile, tablet, and desktop support

---

## ðŸš€ Quick Start

### Install

```bash
pip install servemd
```

### Run

```bash
# Serve docs from current directory
servemd

# Or specify a directory
servemd ./my-docs
```

Visit **http://localhost:8080** â€” your documentation is live.

### Alternative: Docker

```bash
# Quick start - mount your docs
docker run -p 8080:8080 -v $(pwd)/docs:/app/__docs__ jberends/servemd:latest

# Or build custom image with docs baked in
FROM jberends/servemd:latest
COPY ./my-docs/ /app/__docs__/
```

**See [DOCKER_README.md](DOCKER_README.md)** for complete Docker usage guide.

### Alternative: uvx (no install)

```bash
uvx servemd ./my-docs
```

ðŸ“š **[Complete Setup Guide â†’](docs/quick-start-user.md)**

### For Contributors

```bash
git clone https://github.com/servemd/servemd
cd servemd
uv sync
uv run python -m docs_server
```

---

## ðŸ¤– AI-Native: llms.txt & MCP Support

servemd automatically serves your docs in AI-friendly formats:

| Endpoint | Purpose | Audience | Context Size |
|----------|---------|----------|--------------|
| `/{page}.html` | Rendered HTML with navigation | Humans | N/A |
| `/{page}.md` | Raw markdown | AI/LLMs | Per-page |
| `/llms.txt` | Documentation index | AI assistants | Small (~5KB) |
| `/llms-full.txt` | Complete context (all pages) | AI deep context | Large (~500KB+) |
| `/mcp` | **Interactive queries** | **AI (MCP clients)** | **Minimal (250x less)** |

### llms.txt - Static Index

**Example:** Give an AI assistant your docs:
```
"Read my documentation at https://docs.example.com/llms.txt"
```

The AI gets a structured index with absolute URLs to every page. For complete context, use `/llms-full.txt` which includes all page content inline.

### MCP - Interactive Queries (Recommended for AI)

**Model Context Protocol** provides on-demand documentation access with **250x less context**:

```json
POST /mcp
{
  "method": "tools/call",
  "params": {
    "name": "search_docs",
    "arguments": {
      "query": "authentication",
      "limit": 5
    }
  }
}
```

**Benefits:**
- ðŸŽ¯ **Precise** â€” AI queries only what it needs
- âš¡ **Fast** â€” No sending entire documentation every time
- ðŸ’° **Cost-effective** â€” 250x less tokens than llms-full.txt
- ðŸ” **Smart** â€” Full-text search with Whoosh

**Available MCP Tools:**
- `search_docs` â€” Semantic search across documentation
- `get_doc_page` â€” Retrieve specific pages with section filtering
- `list_doc_pages` â€” List all available pages by category

**See [MCP Integration Guide](docs/features/mcp.md)** for details.

---

## âœ¨ Key Features

### For Humans
- ðŸŽ¨ Nuxt UI-inspired three-column layout (sidebar, content, TOC)
- ðŸŽ¨ Syntax highlighting with Pygments
- ðŸŽ¨ Responsive design (mobile, tablet, desktop)
- ðŸŽ¨ Dark mode ready
- âœ… Tables, task lists, footnotes, Mermaid diagrams

### For AI
- ðŸ¤– **llms.txt** â€” Structured documentation index (5KB)
- ðŸ¤– **llms-full.txt** â€” Complete context export (500KB+)
- ðŸ”Œ **MCP endpoint** â€” Interactive queries via Model Context Protocol
  - **250x less context** than llms-full.txt
  - Full-text search with Whoosh
  - On-demand page retrieval
  - Section filtering
- ðŸ¤– Automatic link transformation to absolute URLs
- ðŸ¤– Curated or auto-generated indexes

### For Developers
- âš¡ Fast â€” disk caching, <5ms cached responses
- ðŸ”¥ Hot reload in debug mode
- ðŸ”§ Zero configuration required
- ðŸ Python 3.11-3.14, FastAPI, Pydantic
- ðŸ§ª 208 tests, 100% passing

---

## ðŸ“ File Structure

Your documentation needs just 3 required files:

```
docs/
â”œâ”€â”€ index.md       # Homepage (required)
â”œâ”€â”€ sidebar.md     # Navigation (required)
â”œâ”€â”€ topbar.md      # Top bar (required)
â”œâ”€â”€ llms.txt       # AI index (optional)
â””â”€â”€ your-content.md # Your pages
```

---

## âš™ï¸ Configuration

Configure via environment variables:

```bash
DOCS_ROOT=./docs                 # Documentation directory
CACHE_ROOT=./__cache__           # Cache directory
PORT=8080                        # Server port
DEBUG=true                       # Enable debug mode
BASE_URL=https://docs.site.com   # Base URL for llms.txt
MCP_ENABLED=true                 # Enable MCP endpoint (default: true)
MCP_RATE_LIMIT_REQUESTS=120      # MCP rate limit (requests per window)
MCP_RATE_LIMIT_WINDOW=60         # MCP rate limit window (seconds)
```

See [Configuration Guide](docs/configuration.md) for details.

---

## ðŸŽ¯ Use Cases

**servemd** is perfect for:

- **SaaS Documentation** â€” Customer-facing support docs with AI assistant integration
- **Open Source Projects** â€” Self-hosted, beautiful docs
- **Internal Teams** â€” Company knowledge bases and wikis
- **API Documentation** â€” REST/GraphQL API docs
- **Technical Writing** â€” Blogs and tutorials

### ðŸ“˜ Deployment

| Method | Best For |
|--------|----------|
| [Local Development](docs/deployment/local-development.md) | Development, previewing |
| [Docker](docs/deployment/docker.md) | Production, CI/CD |
| [Cloud Platforms](docs/deployment/cloud-platforms.md) | Heroku, Railway, Fly.io, DigitalOcean |
| [Kubernetes](docs/deployment/kubernetes.md) | k8s, k3s, Helm charts |

### ðŸ› ï¸ Examples

Check **[examples/](examples/)** for ready-to-use templates:
- `Dockerfile.user-template` â€” Custom Docker image
- `docker-compose.user.yml` â€” Docker Compose setup
- `k8s-simple.yaml` â€” Kubernetes deployment

---

## ðŸ—ï¸ Architecture

Clean, modular FastAPI application:

```
src/docs_server/
â”œâ”€â”€ config.py           # Settings & environment
â”œâ”€â”€ helpers.py          # Utilities & navigation
â”œâ”€â”€ caching.py          # Smart caching
â”œâ”€â”€ markdown_service.py # Markdown rendering
â”œâ”€â”€ llms_service.py     # LLMs.txt generation
â”œâ”€â”€ templates.py        # HTML templates
â”œâ”€â”€ main.py             # FastAPI routes
â””â”€â”€ mcp/                # Model Context Protocol
    â”œâ”€â”€ server.py       # MCP JSON-RPC handler
    â”œâ”€â”€ tools.py        # MCP tools (search, get, list)
    â”œâ”€â”€ search.py       # Full-text search with Whoosh
    â””â”€â”€ indexer.py      # Documentation indexing
```

---

## ðŸ§ª Testing

```bash
uv run pytest tests/ -v

# 208 tests, 100% passing âœ…
```

---

## ðŸ”§ Development

```bash
git clone https://github.com/servemd/servemd
cd servemd
uv sync --group dev
uv run pytest tests/ -v
DEBUG=true uv run python -m docs_server
```

---

## ðŸ“Š Performance

| Endpoint | First Request | Cached |
|----------|---------------|--------|
| Rendered HTML | 50-100ms | <5ms |
| Raw Markdown | <10ms | <10ms |
| LLMs.txt | 100-200ms | <5ms |

---

## ðŸ“– Documentation

- **[Quick Setup](docs/quick-setup.md)** â€” Get running in 5 minutes
- **[Markdown Features](docs/features/markdown.md)** â€” Tables, code blocks, diagrams
- **[LLMs.txt Guide](docs/features/llms-txt.md)** â€” AI assistant integration
- **[MCP Integration](docs/features/mcp.md)** â€” Interactive queries for LLMs
- **[Navigation](docs/features/navigation.md)** â€” Sidebar and topbar configuration
- **[Configuration](docs/configuration.md)** â€” Environment variables
- **[API Reference](docs/api/endpoints.md)** â€” HTTP endpoints

---

## ðŸ™‹ Support

- ðŸ“– **Documentation**: [docs.servemd.dev](https://docs.servemd.dev) or run locally
- ðŸ› **Issues**: [GitHub Issues](https://github.com/servemd/servemd/issues)
- ðŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/servemd/servemd/discussions)

---

## ðŸ“œ License

MIT License â€” use freely for any project.

---

## ðŸŽ‰ Get Started Now

```bash
pip install servemd
servemd ./my-docs
```

Visit **http://localhost:8080** â€” beautiful docs for humans, structured context for AI.

---

<div align="center">

**servemd** â€” Serve docs to humans and AI

Built with Python, FastAPI, and Markdown

[Documentation](https://docs.servemd.dev) Â· [PyPI](https://pypi.org/project/servemd/) Â· [GitHub](https://github.com/servemd/servemd)

</div>
